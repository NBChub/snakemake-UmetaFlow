{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Data information:`\n",
    "\n",
    "- `x` genomes (strains) \n",
    "- `x` different treatments (growth conditions)\n",
    "- `x3` replicates\n",
    "- plus `x` negative controls (referred to as \"blanks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Prepare Matrix for PCA`\n",
    "- Cluster the different strains (groupby genome ID) and calculate the sum (0.0 no feature detected in all 3 treatments in all 3 replicates - 9.0 feature detected in all 3 treatments in all 3 replicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= os.path.join(\"results\", \"statistical_analysis\")\n",
    "isExist= os.path.exists(path)\n",
    "if not isExist:\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can group according to what you would like to compare. E.g. genomes or treatments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix= pd.read_csv(os.path.join(\"results\", \"Requantified\", \"FeatureMatrix.tsv\"), sep=\"\\t\")\n",
    "Matrix= Matrix.set_index([\"mz\", \"RT\"])\n",
    "Matrix= Matrix.fillna(0)\n",
    "Matrix= Matrix.where(Matrix==0, 1) #replace with 1.0 all intensities that are not 0 (NaN)\n",
    "Matrix= Matrix.sort_index(axis=1)\n",
    "cols= Matrix.columns\n",
    "Matrix= Matrix.transpose()\n",
    "Matrix= Matrix.reset_index()\n",
    "Matrix['genomeID']=Matrix['index'].str.extract(r'(NBC_?\\d*)')\n",
    "Matrix['genomeID_MDNA']=Matrix['index'].str.extract(r'(MDNAWGS?\\d*|MDNA_WGS_?\\d*)')\n",
    "Matrix['genomeID']=Matrix['genomeID'].fillna(Matrix['genomeID_MDNA'])\n",
    "Matrix['treatments']= Matrix['index'].str.extract(r'(ISP2|DNPM|FPY12)')\n",
    "Matrix= Matrix.drop(columns=[\"genomeID_MDNA\"])\n",
    "Matrix=Matrix.set_index([\"index\"])\n",
    "Matrix= Matrix.drop(columns=[\"genomeID\"]) #or treatments\n",
    "Grouped= Matrix.groupby(\"treatments\").sum() #or genome ID\n",
    "Grouped= Grouped.transpose()\n",
    "Grouped= Grouped.reset_index()\n",
    "Grouped.to_csv(os.path.join(path, \"Matrix_Grouped.csv\"), sep=\"\\t\", index=None)\n",
    "Grouped= Grouped.set_index([\"mz\", \"RT\"])\n",
    "Grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureMatrix= pd.read_csv(os.path.join(path, \"Matrix_Grouped.csv\"), sep='\\t')\n",
    "FeatureMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `2) PCA and outlier analysis`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `(i) Data pre-treatment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install seaborn\n",
    "!conda install sklearn\n",
    "!conda install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as shc \n",
    "\n",
    "from collections import Counter\n",
    "#from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.model_delection import ParameterGrid\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix= pd.read_csv(os.path.join(path,\"Matrix_Grouped.csv\"), sep=\"\\t\")\n",
    "Matrix= Matrix.set_index([\"mz\", \"RT\"])\n",
    "Matrix= Matrix.transpose()\n",
    "Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Centering`\n",
    "    - Import the grouped csv files and perform mean centering to focus on the fluctuating part of the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix_centered = Matrix.apply(lambda x: x-x.mean())\n",
    "Matrix_centered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explore if you have outliers with unbiasted skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix_centered.skew(axis=0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix_centered.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Standardization`\n",
    "    - This will make sure that all the features are centred around the mean value with a standard deviation value of 1. This is the best to use if your feature is normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def variance(data, ddof=0):\n",
    "    n = len(data)\n",
    "    mean = sum(data) / n\n",
    "    return sum((x - mean) ** 2 for x in data) / (n - ddof)\n",
    "\n",
    "\n",
    "def stdev(data):\n",
    "    var = variance(data)\n",
    "    std_dev = math.sqrt(var)\n",
    "    return std_dev\n",
    "    \n",
    "Matrix_standardized= Matrix_centered.apply(lambda x: (x-x.mean()) / stdev(x))\n",
    "Matrix_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= Matrix_centered\n",
    "scaled_data = Matrix_standardized\n",
    "f, ax = plt.subplots(1,2)\n",
    "sns.distplot(x, ax=ax[0], color='y')\n",
    "ax[0].set_title(\"Original data\")\n",
    "sns.distplot(scaled_data, ax=ax[1], color='g')\n",
    "ax[1].set_title(\"Scaled data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, as you know, the principle components of the data are the\n",
    "# dimensions along which the data varies the most. The data\n",
    "# here is 4 dimensional (since there are 4 features) and thus\n",
    "# there are 4 total principle components.\n",
    "\n",
    "# The first principle component will be the line along which the\n",
    "# data varies the most. The second will be the line along with the\n",
    "# data varies the second monst, and so on. The sum of the variances\n",
    "# of all the principle components will be the entire variance of the\n",
    "# dataset.\n",
    "\n",
    "# Okay, so let's use the PCA function to get all 4 principle components.\n",
    "\n",
    "# As always with the sklearn package, we first have to create and save\n",
    "# a function \"object\":\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# We can actually use this function object to get the first 4 principle\n",
    "# components of any data. Let's do it for our data:\n",
    "data_pca = pca.fit_transform(Matrix_standardized)\n",
    "\n",
    "# Now that we've called fit_transform, the pca object some attributes\n",
    "# that includes the data points transformed along the principle components.\n",
    "# So let's plot the data points along the first two principle components.\n",
    "# We'll use the matplotlib.pyplot package to do this:\n",
    "plt.scatter(data_pca[:, 0], data_pca[:, 1])\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.title(\"Our data along the first two PCs\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# The explained variance is also included in the pca object.\n",
    "# So we can plot that as well:\n",
    "plt.plot(pca.explained_variance_ratio_,\n",
    "        label=\"Percent of variance explained for each PC\")\n",
    "plt.plot([sum(pca.explained_variance_ratio_[:i]) for i in range(1, 5)],\n",
    "        label=\"Cumulative percent of variance explained\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(range(4), [\"%0.0f\" % i for i in range(1, 5)])\n",
    "plt.ylabel(\"%\")\n",
    "plt.xlabel(\"PC\")\n",
    "plt.title(\"Variance explained by each PC\")\n",
    "plt.show()\n",
    "\n",
    "#make a plot where you zoom in\n",
    "#volcano plots\n",
    "# So now we see empirical evidence that the sum of all 4 variances\n",
    "# equals the total variance in the data set (shown by the orange line) :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive PCA plots inspired from https://www.kaggle.com/maniyar2jaimin/interactive-plotly-guide-to-pca-lda-t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as tls\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the 3 dimensionality reduction methods\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = Matrix_standardized.values\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "# Calculating Eigenvectors and eigenvalues of Cov matirx\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eigh(cov_mat) #ASK\n",
    "# Create a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "# Sort the eigenvalue, eigenvector pair from high to low\n",
    "eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
    "\n",
    "# Calculation of Explained Variance from the eigenvalues\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Scatter(\n",
    "    x=list(range(10)),\n",
    "    y= cum_var_exp,\n",
    "    mode='lines+markers',\n",
    "    hoverinfo= \"all\",\n",
    "    name=\"'Cumulative Explained Variance'\",\n",
    "    line=dict(\n",
    "        shape='spline',\n",
    "        color = 'goldenrod'\n",
    "    )\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x=list(range(10)),\n",
    "    y= var_exp,\n",
    "    hoverinfo= \"all\",\n",
    "    mode='lines+markers',\n",
    "    name=\"'Individual Explained Variance'\",\n",
    "    line=dict(\n",
    "        shape='linear',\n",
    "        color = 'black'\n",
    "    )\n",
    ")\n",
    "fig = tls.make_subplots(insets=[{'cell': (1,1), 'l': 0.7, 'b': 0.3}],\n",
    "                          print_grid=True)\n",
    "\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.layout.xaxis = dict(range=[0, 9.5])\n",
    "fig.layout.yaxis = dict(range=[0, 110])\n",
    "\n",
    "py.iplot(fig, filename='inset example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= Matrix_standardized.values\n",
    "\n",
    "# Standardising the values\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Call the PCA method with 50 components. \n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_std)\n",
    "X_5d = pca.transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace0 = go.Scatter(\n",
    "    x = X_5d[:,0],\n",
    "    y = X_5d[:,1],\n",
    "    name = \"Target\",\n",
    "    hoverinfo = 'all',\n",
    "    mode = 'markers',\n",
    "    showlegend = False,\n",
    "    marker = dict(\n",
    "        size = 8,\n",
    "        colorscale ='Jet',\n",
    "        showscale = False,\n",
    "        line = dict(\n",
    "            width = 2,\n",
    "            color = 'rgb(255, 255, 255)'\n",
    "        ),\n",
    "        opacity = 0.8\n",
    "    )\n",
    ")\n",
    "data = [trace0]\n",
    "\n",
    "layout = dict(title = 'PCA (Principal Component Analysis)',\n",
    "              hovermode= 'closest',\n",
    "              yaxis = dict(zeroline = False),\n",
    "              xaxis = dict(zeroline = False),\n",
    "              showlegend= True\n",
    "             )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename='styled-scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1=Matrix_standardized.quantile(0.25)\n",
    "Q3=Matrix_standardized.quantile(0.75)\n",
    "IQR=Q3-Q1\n",
    "upper_val= Q3+1.5*IQR #most frequently occuring features    \n",
    "lower_val= Q1-1.5*IQR #most rarely occuring features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only investigate the rarely occuring metabolites(lower_val):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_val.to_csv(os.path.join(path, \"lower_val.csv\"), sep=\"\\t\")\n",
    "lower_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix_standardized= Matrix_standardized.transpose()\n",
    "Matrix_standardized= Matrix_standardized.reset_index()\n",
    "Matrix_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_val= pd.read_csv(os.path.join(path, \"lower_val.csv\"), sep=\"\\t\")\n",
    "lower_val= lower_val.rename(columns={\"0\":\"rarest_features\"})\n",
    "Matrix_scaled= pd.merge(Matrix_standardized, lower_val, on=[\"mz\", \"RT\"])\n",
    "Matrix_scaled= Matrix_scaled.set_index([\"mz\", \"RT\"])\n",
    "Matrix_scaled= Matrix_scaled.round(decimals=5)\n",
    "Matrix_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix_standardized=Matrix_standardized.set_index([\"mz\", \"RT\"])\n",
    "cols= Matrix_standardized.columns\n",
    "outliers=Matrix_scaled\n",
    "\n",
    "for col in cols:\n",
    "    for idx in outliers.index:\n",
    "        if outliers[\"Lower_limit\"][idx]<= outliers[col][idx]:\n",
    "            outliers.loc[idx, col]=np.nan\n",
    "\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers= outliers.drop(columns=[\"Upper_limit\", \"Lower_limit\"])\n",
    "outliers= outliers.dropna(how=\"all\")\n",
    "outliers= outliers.transpose()\n",
    "outliers= outliers.dropna(how=\"all\")\n",
    "outliers= outliers.transpose()\n",
    "outliers.to_csv(os.path.join(path, \"outliers.csv\"), sep=\"\\t\")\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Outlier GNPS annotation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(os.path.join(\"resources\", \"MS2_LIBRARYSEARCH_all_identifications.tsv\"), sep='\\t', encoding='latin-1')\n",
    "df.drop(df.index[df['IonMode'] == \"negative\"], inplace=True)\n",
    "df.drop(df.index[df['MZErrorPPM'] > 20.0], inplace=True)\n",
    "GNPS=df.drop(columns=[\"PI\", \"Adduct\", \"IonMode\", \"Organism\", \"MZErrorPPM\", \"SpecMZ\"])\n",
    "GNPS=GNPS.rename(columns= {\"RT_Query\": \"RetentionTime\"})\n",
    "GNPS=GNPS.drop_duplicates(subset=\"Compound_Name\", keep='first')\n",
    "GNPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.insert(0, 'GNPS_IDs', '')\n",
    "\n",
    "for i, mz, rt in zip(outliers.index, outliers['mz'], outliers['RT']):\n",
    "    hits = []\n",
    "    for name, GNPS_mz, GNPS_rt, in zip(GNPS['Compound_Name'], GNPS['Precursor_MZ'], GNPS['RetentionTime']):\n",
    "        mass_delta = (abs(GNPS_mz-mz)/GNPS_mz)*1000000.0 if GNPS_mz != 0 else np.nan\n",
    "        if (GNPS_rt >= rt-30.0) & (GNPS_rt <= rt+30.0) & (mass_delta<= 20.0):\n",
    "            hit = f'{name}'\n",
    "            if hit not in hits:\n",
    "                hits.append(hit)\n",
    "    outliers['GNPS_IDs'][i] = ' ## '.join(hits)\n",
    "\n",
    "outliers.to_csv(os.path.join(\"results\", \"annotations\", \"GNPS_annotated_outliers.tsv\"), sep='\\t', index = False)\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers= outliers[outliers.GNPS_IDs == '']\n",
    "outliers= outliers.drop(columns= \"GNPS_IDs\")\n",
    "outliers= outliers.set_index([\"mz\", \"RT\"])\n",
    "outliers_tocsv= outliers.reset_index()\n",
    "outliers_tocsv.to_csv(os.path.join(\"results\", \"annotations\", \"outliers_unknowns.tsv\"), sep=\"\\t\", index =None)\n",
    "outliers"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edde62aa2661007f0756e9790e7a328c288a583bf6ce768a355147dac67c8db8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
